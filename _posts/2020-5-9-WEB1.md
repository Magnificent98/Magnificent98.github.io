---
layout: post
title: Web Scraping(I)
category: Web
date: 2020-5-9
---

# 本章作为爬虫的开始章，介绍一下爬虫的基本概念以及个人守则

## 1.爬虫基础

### 1.1 合法性
<p>&nbsp;&nbsp; 由于网络爬虫爬取的是数据，而数据是当下最为敏感的东西之一。哪些东西可以爬取，哪些东西不可以爬取是我们首先要注意的。然而目前在这一方面还没有完整的法律约束，是靠互联网自己的游戏规则进行约束的。一般认为，当抓取的数据是现实生活中的真实数据(营业地址，电话清单)时是合法的；而原创数据(意见和评论)通常会受到版权的限制，不能爬取。 </p><br/>
<p>&nbsp;&nbsp; 其实在理想状态下，每个提供内容的网站都应该有自己的API，以结构化的格式共享它们的数据。现实生活中有的网站确实提供了API，但是通常会限制可以抓取的数据以及访问数据的频率。所以仅仅依赖API访问我们需要的数据是远远不够的。我们需要网络爬虫的帮助。 </p><br/>
<p>&nbsp;&nbsp; 最后，当我们抓取网站数据的时候，要清楚自己是网站的访客，要自觉地约束自己的抓取行为，不然可能会被封禁IP或采取更进一步的法律行动。 </p><br/>

### 1.2 背景调研
&nbsp;&nbsp;背景调研是对要爬取的网站的规模以及结构的认识。

#### 1.2.1 robots.txt
<p>&nbsp;&nbsp; 大多数网站都会定义robots.txt文件，这就是互联网自己的“游戏规则”。这个文件让爬虫了解网站的限制，而我们作为编写者，应该遵守这些限制。访问网站的robots.txt文件一般是在域名之后加上/robots.txt即可。 </p><br/>

```txt
访问robots.txt示例
url = https://www.baidu.com/robots.txt

User-agent: Baiduspider
Disallow: /baidu
Disallow: /s?
Disallow: /ulink?
Disallow: /link?
Disallow: /home/news/data/
Disallow: /bh

User-agent: Googlebot
Disallow: /baidu
Disallow: /s?
Disallow: /shifen/
Disallow: /homepage/
Disallow: /cpro
Disallow: /ulink?
Disallow: /link?
Disallow: /home/news/data/
Disallow: /bh

User-agent: MSNBot
Disallow: /baidu
Disallow: /s?
Disallow: /shifen/
Disallow: /homepage/
Disallow: /cpro
Disallow: /ulink?
Disallow: /link?
Disallow: /home/news/data/
Disallow: /bh

... ...

```
<br/>
<p>&nbsp;&nbsp; User-agent是http协议的一部分，网站会通过这一字段判断用户的访问方式。如果不是浏览器访问，那么大多数网站会禁止访问。所以我们要在爬虫中加入这个字段，伪装成浏览器访问。Disallow 定义了很多不能访问的地址。如果访问，则爬虫会被封禁。 </p><br/>

#### 1.2.2 Sitemap
<p>&nbsp;&nbsp; 网站的sitemap提供了网站地图，帮助爬虫定位网站最新的内容。一般是以xml的格式呈现的。虽然sitemap提供了一种爬取网站的有效方式，但是经常存在缺失，过期，不完整的问题。 </p><br/>

```txt
url = https://www.sunbon88.com/sitemap.xml
<urlset xmlns="http://www.sitemaps.org/schemas/sitemap/0.9" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.sitemaps.org/schemas/sitemap/0.9 http://www.sitemaps.org/schemas/sitemap/0.9/sitemap.xsd">
<url>
<loc>https://www.sunbon88.com/sbnzx.html</loc>
<priority>1.0</priority>
<changefreq>weekly</changefreq>
<lastmod>2018-04-25T10:25:22+08:00</lastmod>
</url>
<url>
<loc>https://www.sunbon88.com/gysbn.html</loc>
<priority>1.0</priority>
<changefreq>weekly</changefreq>
<lastmod>2018-04-25T10:25:22+08:00</lastmod>
</url>
... ...

```
<br/>

#### 1.2.3 估算网站大小
<p>&nbsp;&nbsp; 俗话说，站在巨人的肩膀上我们可以看得更远。在我们要估算一个目标网站大小的时候，可以参考已经爬取过它的爬虫的结果。这里说到的是Google爬虫。我们可以在Google中搜索‘site:magzhang.xyz’，然后看看条目数，基本上就是这个网站的大小了。 </p><br/>

<div align="center">
<img src="{{site.url}}{{site.baseurl}}{{site.assets_path}}/img/crawl1.png" width="50%" height="30%"/>
</div>
<br/><br/>


#### 1.2.4 识别网站所用的技术
<p>&nbsp;&nbsp; 这里使用python的builtwith库。 </p><br/>

```python
import builtwith

print builtwith.parse('http://magzhang.xyz')
# {u'javascript-graphics': [u'MathJax']}

```
<br/>

#### 1.2.5 寻找网站的所有者
<p>&nbsp;&nbsp; 这里使用python的whois库。 </p><br/>

```python
import whois

print whois.whois('http://magzhang.xyz')
# 从结果中可以看域名的归属是谁，然后指定爬取的策略

```
<br/>

### 1.3 爬虫实验

<p>&nbsp;&nbsp; 爬虫完成的过程其实就是下载网页，然后寻找自己要的内容。重点是怎样让计算机按照我们意愿，连续地搜寻网页。 </p><br/>

#### 1.3.1 下载网页

```python
#encoding=utf-8
import urllib2

def download(url, user_agent='wswp', num_retries=2):
    print 'Downloading: ', url
    # 设置用户代理，伪装成浏览器
    headers = {'User-agent': user_agent}
    request = urllib2.Request(url, headers=headers)
    try:
        html = urllib2.urlopen(request).read()
    except urllib2.URLError as e:
        print 'Download error: ', e.reason
        html = None
        # 如果代码是5xx，是服务端出现问题，可以重新尝试。
        if num_retries > 0:
            if hasattr(e, 'code') and 500 <= e.code < 600:
                return download(url, num_retries-1)
    return html


if __name__ == '__main__':
    print download('https://magzhang.xyz/PY5/')

```
<br/>

#### 1.3.2 网站地图爬虫
<p>&nbsp;&nbsp; 当站点拥有完整的sitemap的时候，可以通过sitemap来下载所有的网页。 </p><br/>

```python
def crawl_sitemap(url, user_agent='wswp', num_retries=2):
    # 下载sitemap
    sitemap = download(url, user_agent, num_retries)
    # 从<loc></loc>中找到所有的地址
    links = re.findall('<loc>(.*?)</loc>', sitemap)
    for link in links:
        html = download(link, user_agent, num_retries)
        # 下面从获得的html中爬取数据
        # ...

```
<br/>

#### 1.3.3 ID遍历爬虫
<p>&nbsp;&nbsp; ID爬虫利用的是Web服务器的优化机制。当页面是使用连续的ID命名的时候，我们可以简化下载的过程。 </p><br/>

```python
import itertools
max_errors = 5  # 设置最大错误次数(有时候序号可能不连续)
num_errors = 0
for page in itertools.count(1):
    url = r'http://example.webscraping.com/places/default/view/%d' % page
    html = download(url)
    if html is None:
        num_errors += 1
        if num_errors > max_errors
            break
    else:
        # scrape the result
        num_errors = 0
    
```
<br/>

#### 1.3.4 链接爬虫
<p>&nbsp;&nbsp; 这种方式是使用正则表达式匹配下载规律，只下载自己感兴趣的页面，寻找信息。 </p><br/>

```python
def link_crawler(seed_url, link_regex):
    # 链接的"层序遍历"
    crawl_queue = [seed_url]
    # 建立访问过页面的集合，防止重复访问
    seen = set(crawl_queue)
    while crawl_queue:
        url = crawl_queue.pop()
        html = download(url)
        for link in get_links(html):
            if re.match(link_regex, link):
                # 将相对地址转化为绝对地址
                link = urlparse.urljoin(seed_url, link)
                if link not in seen:
                    seen.add(link)
                    crawl_queue.append(link)

def get_links(html):
    webpage_regex = re.compile('<a[^>]+href=["\'](.*?)["\']', re.IGNORECASE)
    return webpage_regex.findall(html)

```
<br/>

<p>&nbsp;&nbsp; 举个例子，对于如下结构的网页： </p><br/>

```txt
http://example.webscraping.com/places/default/index/1
http://example.webscraping.com/places/default/index/2

http://example.webscraping.com/places/default/view/Afghanistan-1
http://example.webscraping.com/places/default/view/Aland-Islands-2
```

<p>&nbsp;&nbsp; 我们可以输入一个根url http://example.webscraping.com/places/default，然后使用正则表达式 /(index|view)去匹配不同的网页。link_crawler应该提供的功能是补全链接以及网址陷入死循环(网页之间跳来跳去)。 </p><br/>


#### 1.3.4 高级功能
<p>&nbsp;&nbsp; 基础的下载与爬链接完成以后，我们给它加一点高级功能。</p><br/>

1.解析robots.txt

```python
# 使用自带的模块分析当前的useragent是否能访问当前的网页
# 可以集成到之前写的爬虫中，来遵守robots.txt协议
import robotparser
if __name__ == '__main__':
    rp = robotparser.RobotFileParser()
    rp.set_url('http://example.webscraping.com/robots.txt')
    rp.read()
    print rp.can_fetch(useragent='BadCrawler', url='http://example.webscraping.com')

```
<br/>

2.支持代理

```python
# 这个是满血版的download函数
# 可以设置代理，用户代理，重试次数
# 之后可以无脑搬来用
def download(url, user_agent='wswp', proxy=None, retries=2):
    '''输入：url, user_agent, proxy, retries
        返回：打开的html内容
    '''
    print 'Downloading: ', url
    header = {'User-agent': user_agent}
    request = urllib2.Request(url, headers=header)
    opener = urllib2.build_opener()
    if proxy:
        proxy_params = {urlparse.urlparse(url).scheme: proxy}
        opener.add_handler(urllib2.ProxyHandler(proxy_params))
    try:
        html = opener.open(request).read()
    except urllib2.URLError as e:
        print 'Download error: ', e.reason
        html = None
        if retries:
            return download(url, user_agent, proxy, retries-1)
    return html

```
<br/>

3.下载限速

```python
# 我们爬取网站的速度过快就会面临封禁的危险
# 我们要在两次下载之间添加延时，对爬虫进行限速
class Throttle:
    def __init__(self, delay):
        self.delay = delay
        # 一个字典，记录每个域名上次访问的时间
        self.domains = {}

    def wait(self, url):
        # netloc是返回域名服务器
        domain = urlparse.urlparse(url).netloc
        # 取出域名上次访问的时间
        last_accessed = self.domains.get(domain)
        if self.delay > 0 and last_accessed is not None:
            # 计算休眠时间
            sleep_sec = self.delay - (datetime.datetime.now() - last_accessed).seconds
            if sleep_sec > 0:
                time.sleep(sleep_sec)
        # 第一次访问则直接存入时间
        self.domains[domain] = datetime.datetime.now()
```
<br/>

4.避免陷阱

```python
# 为了避免爬虫越陷越深，停不下来，我们要设置一个页面最大深度
# 超过这个深度以后，爬虫自动停止
def link_crawler(seed_url, regex, max_depth=2):
    crawl_queue = queue.Queue()
    crawl_queue.put(seed_url)
    # 记录当前页面的深度
    seen = {seed_url: 1}
    while not crawl_queue.empty():
        site = crawl_queue.get()
        depth = seen.get(site)
        # 如果超过指定的深度，则停止
        if depth > max_depth:
            break
        html = download(site)
        links = re.compile(r'<a[^>]+href=["\'](.*?)["\']').findall(html)
        for link in links:
            if re.match(regex, link):
                link = urlparse.urljoin(seed_url, link)
                if link not in seen:
                    crawl_queue.put(link)
                    seen[link] = depth + 1

```
<br/>

### 1.4 第一个爬虫的最终版本

```python
#encoding=utf-8
import urllib2
import urlparse
import datetime
import time
import robotparser
import queue
import re

class Throttle:
    '''limit the speed of the crawler.'''
    def __init__(self, delay):
        self.delay = delay
        self.domains = {}

    def wait(self, url):
        domain = urlparse.urlparse(url).netloc
        last_access = self.domains.get(domain)

        if self.delay > 0 and last_access is not None:
            sleep_sec = self.delay - (datetime.datetime.now() - last_access).seconds
            if sleep_sec > 0:
                print 'delaying...'
                time.sleep(sleep_sec)
        self.domains[domain] = datetime.datetime.now()

class MyCrawler:
    '''a crawler developed by Mag'''
    def __init__(self):
        print 'Welcome to Mag\'s Crawler.'

    def __robotstxt(self, url):
        rp = robotparser.RobotFileParser()
        rp.set_url(url)
        rp.read()
        return rp

    def download_pages(self, url, user_agent='mag', proxy=None, num_retries=2):
        '''download pages.'''
        print 'Downloading: ', url
        header = {'User-agent': user_agent}
        request = urllib2.Request(url, headers=header)
        opener = urllib2.build_opener()
        if proxy:
            proxy_params = {urlparse.urlparse(url).scheme: proxy}
            opener.add_handler(urllib2.ProxyHandler(proxy_params))
        try:
            html = opener.open(request).read()
        except urllib2.URLError as e:
            print 'Download Error:', e.reason
            html = None
            if num_retries:
                if hasattr(e, 'code') and 500 <= e.code < 600:
                    html = self.download_pages(url, user_agent, proxy, num_retries-1)
        return html

    def sitemap_crawler(self, sitemap_url, user_agent='mag', proxy=None, num_retries=2,\
                        robotstxt=None):
        '''accept a url of sitemap and return all the links.
            links are all abide by robots.txt when given the address.
        '''
        sitemap = self.download_pages(sitemap_url, user_agent, proxy, num_retries)
        links = re.compile(r'<loc>(.*?)</loc>').findall(sitemap)
        if robotstxt is None:
            return links
        results = []
        rp = self.__robotstxt(robotstxt)
        for link in links:
            if rp.can_fetch(user_agent, link):
                results.append(link)
        return results

    def link_crawler(self, seed_url, link_regex, user_agent='mag', proxy=None, \
                     num_retries=2, max_depth=10, robotstxt=None, delay=-1):
        '''accept a seed url and link regulation expression.
            return all the matched links.
            links are all abide by robots.txt when given the address.
        '''
        link_queue = queue.Queue()
        link_queue.put(seed_url)
        seen = {seed_url: 1}
        results = [seed_url]
        throttle = Throttle(delay)
        rp = None
        if robotstxt:
            rp = self.__robotstxt(robotstxt)
        while not link_queue.empty():
            site = link_queue.get()
            depth = seen[site]
            if depth > max_depth:
                break
            if (rp and rp.can_fetch(user_agent, site)) or not rp:
                throttle.wait(site)
                html = self.download_pages(site, user_agent, proxy, num_retries)
                for link in re.compile(r'<a[^>]+href=["\'](.*?)["\']', re.IGNORECASE).findall(html):
                    if re.match(link_regex, link):
                        link = urlparse.urljoin(seed_url, link)
                        if link not in seen:
                            results.append(link)
                            link_queue.put(link)
                            seen[link] = depth + 1
            else:
                print 'The current link %s was denied by robots.txt.' % site
        return results


if __name__ == '__main__':
    print 'please use me as a module.'

```
<br/>