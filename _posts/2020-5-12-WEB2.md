---
layout: post
title: Web Scraping(II)
category: Web
date: 2020-5-12
---

# 在上一章节实现了一个可用的爬虫。下面来说说把网页下载下来以后的数据抓取。

## 2. 数据抓取
<p>&nbsp;&nbsp; 下载网页已经会了，下载下来是为了提取数据的。所以这一节说说数据抓取。 </p><br/>

### 2.1 正则表达式
<p>&nbsp;&nbsp; 正则表达式抓取数据是python的一大优势。因为网页都是结构化的。可以利用正则表达式匹配想要得到的数据结构。 </p><br/>

```python
if __name__ == '__main__':
    m = MyCrawler()
    url = 'http://example.webscraping.com/places/default/view/United-Kingdom-239'
    html = m.download_pages(url)
    print re.compile('<tr id="places_area__row">.*?<td\s*class=["\']w2p_fw["\']>(.*?)</td>').findall(html)

```
<br/>
原网页内容为：

```html
<tr id="places_national_flag__row">
    <td class="w2p_fl">
        <label class="readonly" for="places_national_flag" id="places_national_flag__label">National Flag: </label>
    </td>
    <td class="w2p_fw">
        <img src="/places/static/images/flags/gb.png" />
    </td>
    <td class="w2p_fc"></td>
</tr>
<tr id="places_area__row">
    <td class="w2p_fl">
        <label class="readonly" for="places_area" id="places_area__label">Area: </label>
    </td>
    <td class="w2p_fw">244,820 square kilometres</td>
</tr>
...
```
<br/>
所以这里抓取到的是国家的面积数据。<br/><br/>

<p>&nbsp;&nbsp; 可见正则表达式为我们的抓取数据提供了比较快捷的方法。但是正则表达式也有缺点，那就是十分脆弱。一旦网页发生一点变化，那么正则表达式可能就失效了。所以，在确定完全不会有变化的网页中，或者只是一次性爬取数据的时候才能使用这种方法。 </p><br/>

### 2.2 Beautiful Soup
<p>&nbsp;&nbsp; BeautifulSoup是一个Python模块。这个模块可以解析网页，并且提供定位内容的便捷接口。 </p><br/>

```python
# BeautifulSoup的网页补全功能
from bs4 import BeautifulSoup
broken_html = '<ul class=country><li>Area<li>Population</ul>'
# 在这一步解析网页
soup = BeautifulSoup(broken_html, 'html.parser')
fixed_html = soup.prettify()
print fixed_html
# 原网页：
#<ul class=country><li>Area<li>Population</ul>

# 修复网页：
# <ul class="country">
#   <li>
#   Area
#   <li>
#    Population
#   </li>
#  </li>
# </ul>
```
<br/>

BeautifulSoup模块还可以解析网页。

```python
# 使用BS解析网页，提取自己需要的信息
if __name__ == '__main__':
    m = MyCrawler()
    url = 'http://example.webscraping.com/places/default/view/United-Kingdom-239'
    html = m.download_pages(url)
    soup = BeautifulSoup(html, 'html.parser')
    tr = soup.find(attrs={'id': 'places_area__row'})
    # <tr id="places_area__row"><td class="w2p_fl"><label class="readonly" for="places_area" id="places_area__label">Area: </label></td><td class="w2p_fw">244,820 square kilometres</td><td class="w2p_fc"></td></tr>
    td = tr.find(attrs={'class': 'w2p_fw'})
    # <td class="w2p_fw">244,820 square kilometres</td>
    area = td.text
    print area
    # 244,820 square kilometres

```
<br/>
<p>&nbsp;&nbsp; 可见beautifulsoup具有更强的鲁棒性。就算网页发生一些小的变化，使用beautifulsoup也可以轻松应对。</p><br/>


### 2.3 LXML
<p>&nbsp;&nbsp; LXML也是一个python模块，使用C语言编写，解析速度大于BeautifulSoup。 </p><br/>

```python
# LXML的网页补全功能
import lxml.html
broken_html = '<ul class=country><li>Area<li>Population</ul>'
# 在这一步解析网页
tree = lxml.html.fromstring(broken_html)
fixed_html = lxml.html.tostring(tree, pretty_print=True)
print fixed_html

```
<br/>

LXML同样也可以解析网页，寻找目标，使用的是CSS选择器。

```python
if __name__ == '__main__':
    m = MyCrawler()
    url = 'http://example.webscraping.com/places/default/view/United-Kingdom-239'
    html = m.download_pages(url)
    tree = lxml.html.fromstring(html)
    # css选择器语法
    # tr(标签名)#places_area__row(标签下ID为...) >(属于tr的子标签) td(标签名).w2p_fw(class为w2p_fw)
    td = tree.cssselect('tr#places_area__row > td.w2p_fw')[0]
    area = td.text_content()
    print area

```
<br/>
cssselector的语法如下：

| 语句 | 含义 |
|---|---|
| * | 选择所有标签 |
|---|---|
| a | 选择标签 \<a> |
|---|---|
| .link | 选择 class='link'的元素 |
|---|---|
| a.link | 选择 class='link'的标签\<a> |
|---|---|
| a#home | 选择 ID=‘home’的标签\<a> |
|---|---|
| a > span | 选择父标签为\<a>的\<span>标签 |
|---|---|
| a span | 选择标签\<a>内的\<span>标签 |
|---|---|
| a[title=Home] | 选择 title=Home的标签\<a> |

<br/>

### 2.4 性能对比
<p>&nbsp;&nbsp; 对以上三种方法进行性能对比，可以得出以下结论： </p><br/>

| 抓取方法 | 性能 | 使用难度 | 安装难度 |
|---|---|---|---|
| 正则表达式 | 快 | 困难，稳定性差 | 简单 |
|---|---|---|---|
| Beautifulsoup | 慢 | 简单，稳定性好 | 简单 |
|---|---|---|---|
| LXML | 快 | 简单，稳定性好 | 相对困难 |

<br/>

### 2.5 抓取回调
<p>&nbsp;&nbsp; 之前我们的爬虫只能下载页面，然后就丢弃了数据。接下来我们可以在抓取到页面的基础上，加入一个处理函数，从页面上爬取到我们需要的信息。 </p><br/>

```python
# 顶一个抓取类，把抓取到的信息整理成csv文件
import csv
class ScrapeCallback:
    def __init__(self):
        self.writer = csv.writer(open('countries.csv', 'w'))
        self.fields = ('area', 'population', 'iso', 'country', 'capital', 'continent', 'currency_name', 'languages')
        self.writer.writerow(self.fields)

    # 让对象可被调用
    def __call__(self, url, html):
        if re.search('/view/', url):
            tree = lxml.html.fromstring(html)
            row = []
            for filed in self.fields:
                row.append(tree.cssselect('table > tr#places_{}__row > td.w2p_fw'.format(filed))[0].text_content())
            self.writer.writerow(row)

if __name__ == '__main__':
    m = MyCrawler()
    url = 'http://example.webscraping.com/places/default/view/United-Kingdom-239'
    # 调用方式
    m.link_crawler(url, '/places/default/(view|index)', scrape_callback=ScrapeCallback())

```
<br/>

### 2.6 能爬取数据的爬虫的终极版本

```python
#encoding=utf-8
import urllib2
import urlparse
import datetime
import time
import robotparser
import queue
import re
import csv
import lxml.html

class Throttle:
    '''limit the speed of the crawler.'''
    def __init__(self, delay):
        self.delay = delay
        self.domains = {}

    def wait(self, url):
        domain = urlparse.urlparse(url).netloc
        last_access = self.domains.get(domain)

        if self.delay > 0 and last_access is not None:
            sleep_sec = self.delay - (datetime.datetime.now() - last_access).seconds
            if sleep_sec > 0:
                print 'delaying...'
                time.sleep(sleep_sec)
        self.domains[domain] = datetime.datetime.now()

class MyCrawler:
    '''a crawler developed by Mag'''
    def __init__(self):
        print 'Welcome to Mag\'s Crawler.'


    def __robotstxt(self, url):
        rp = robotparser.RobotFileParser()
        rp.set_url(url)
        rp.read()
        return rp


    def download_pages(self, url, user_agent='mag', proxy=None, num_retries=2):
        '''download pages.'''
        print 'Downloading: ', url
        header = {'User-agent': user_agent}
        request = urllib2.Request(url, headers=header)
        opener = urllib2.build_opener()
        if proxy:
            proxy_params = {urlparse.urlparse(url).scheme: proxy}
            opener.add_handler(urllib2.ProxyHandler(proxy_params))
        try:
            html = opener.open(request).read()
        except urllib2.URLError as e:
            print 'Download Error:', e.reason
            html = None
            if num_retries:
                if hasattr(e, 'code') and 500 <= e.code < 600:
                    html = self.download_pages(url, user_agent, proxy, num_retries-1)
        return html


    def sitemap_crawler(self, sitemap_url, user_agent='mag', proxy=None, num_retries=2,\
                        robotstxt=None, scrape_callback=None):
        '''accept a url of sitemap and scrape all the pages.
            links are all abide by robots.txt when given the address.
        '''
        sitemap = self.download_pages(sitemap_url, user_agent, proxy, num_retries)
        links = re.compile(r'<loc>(.*?)</loc>').findall(sitemap)
        rp = None
        if robotstxt:
            rp = self.__robotstxt(robotstxt)
        if not scrape_callback:
            print 'Nothing to do.'
            return
        for link in links:
            if not rp or (rp and rp.can_fetch(user_agent, link)):
                html = self.download_pages(link)
                print 'scraping ', link
                scrape_callback(link, html)
            else:
                print 'The current link %s was denied by robots.txt.' % link


    def link_crawler(self, seed_url, link_regex, user_agent='mag', proxy=None, \
                     num_retries=2, max_depth=10, robotstxt=None, delay=-1, scrape_callback=None):
        '''accept a seed url and link regulation expression.
            scrape when there is a scrape_callback. ignore the site otherwise.
            links are all abide by robots.txt when given the address.
        '''
        link_queue = queue.Queue()
        link_queue.put(seed_url)
        seen = {seed_url: 1}
        throttle = Throttle(delay)
        rp = None
        if robotstxt:
            rp = self.__robotstxt(robotstxt)
        while not link_queue.empty():
            site = link_queue.get()
            depth = seen[site]
            if depth > max_depth:
                break
            if not rp or (rp and rp.can_fetch(user_agent, link)):
                throttle.wait(site)
                html = self.download_pages(site, user_agent, proxy, num_retries)
                if scrape_callback:
                    print "scraping ", site
                    scrape_callback(site, html)
                else:
                    print "nothing to do with ", site
                for link in re.compile(r'<a[^>]+href=["\'](.*?)["\']', re.IGNORECASE).findall(html):
                    if re.match(link_regex, link):
                        link = urlparse.urljoin(seed_url, link)
                        if link not in seen:
                            link_queue.put(link)
                            seen[link] = depth + 1
                    else:
                        print "bad match ", link
            else:
                print 'The current link %s was denied by robots.txt.' % site


class ScrapeCallback:
    '''customized scraping function.'''
    def __init__(self):
        self.writer = csv.writer(open('countries.csv', 'w'))
        self.fields = ('area', 'population', 'iso', 'country', \
        'capital', 'continent', 'currency_name', 'languages')
        self.writer.writerow(self.fields)


    def __call__(self, url, html):
        if re.search('/view/', url):
            tree = lxml.html.fromstring(html)
            row = []
            for filed in self.fields:
                row.append(tree.cssselect('table > tr#places_{}__row > td.w2p_fw'\
                .format(filed))[0].text_content())
            self.writer.writerow(row)


if __name__ == '__main__':
    m = MyCrawler()
    url = 'http://example.webscraping.com/places/default/view/United-Kingdom-239'
    m.link_crawler(url, '/places/default/(view|index)', scrape_callback=ScrapeCallback())

```