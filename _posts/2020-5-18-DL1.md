---
layout: post
title: Deep Learning(I)
category: Deep Learning
date: 2020-5-18
---

# 本章以及接下来的几章为深度学习的入门章节，为更加高阶的应用打下基础。

## 1. Linear Regression
<p>&nbsp;&nbsp; 我们从线性回归来入门深度学习。线性回归的任务是找到一个从特征空间X到输出空间Y的映射函数。它属于监督学习的一种，并常常用来处理分类预测的问题。线性回归试图学习一个通过属性线性组合来进行预测的函数。 </p><br/>

$$
f(x)=w_1x_1+w_2x_2+w_3x_3+...+b
$$

<p>&nbsp;&nbsp; 下面用一个实例引入。现有若干张照片，有的照片上有阿柴，有的照片上没有。我们需要一个程序来判断这张照片上到底有没有阿柴，也就是这张照片上有阿柴的概率。这种问题可以先进行建模，然后用线性回归来解决。下面是具体的解决方案。 </p><br/>

### 1.1 狗片引入
<p>&nbsp;&nbsp; 对于上述的问题我们要先确定输入输出是什么。一张图片要组织成计算机认识的样子，就要把它的RGB通道的数值拿出来组织成一个列向量X。输出的话是该照片为阿柴的概率，注意是概率所以只能在[0,1]上取值。接下来我们看一看线性回归的具体步骤。 </p><br/>

$$
输入:\;X=[x1,x2,x3,x4....xn]^{T}\\
参数:\;w=[w1,w2,w3,w4,...wn]^{T};\;\;b=0\\
线性组合:\;z=w_1x_1+w_2x_2+...+w_nx_n+b\\
激活函数:\;a=g(z)=sigmoid(z)=\frac{1}{1+e^{-z}}\\
损失函数:\;L(a,y)=-(ylog(a)+(1-y)log(1-a))\\
成本函数:\;J(w,b)=\frac{1}{m}\sum_{i=1}^{m}L(a^{(i)},y^{(i)})
$$

<p>&nbsp;&nbsp; 挨个解释一下。激活函数其实就是把算出的结果映射到0~1上，毕竟要给出的是一个概率。损失函数是计算当前得出的结果与正确结果的差距(单一样本)。这里选择这样的损失函数是因为该函数是凸函数，在定义域上没有局部最优解，为后面的梯度下降提供了必要的理论支持。成本函数则是面向多个(m)样本的。计算的是所有样本损失函数的平均值，学习的目的是使这个成本函数的值最小。下面就说说梯度下降是怎么做的。 </p><br/>

<p>&nbsp;&nbsp; 首先为什么梯度下降可行呢？梯度是方向，是成本函数在当前位置下降速度最快的方向。如果朝着这个方向一直走下去的话，就一定能达到整个函数的最低点。具体怎么做呢？很简单，就是求成本函数对参数们的导数，并且用它来更新参数。</p><br/>

$$
计算图\\
x_1,w_1,x_2,w_2,b\rightarrow z=w_1x_1+w_2x_2+b\rightarrow a=sigmoid(z)\rightarrow L=-(ylog(a)+(1-y)log(1-a))\\
\\
链式求导法则\\
损失函数对预测结果的导:\;\frac{dL}{da}=\frac{1-y}{1-a}-\frac{y}{a}\\
损失函数对线性组合的导:\;\frac{dL}{dz}=\frac{dL}{da}\frac{da}{dz}=a-y\\
损失函数对参数w的导:\;\frac{dL}{dw_n}=\frac{dL}{dz}\frac{dz}{dw_n}=(a-y)x_n\\
损失函数对参数b的导:\;\frac{dL}{db}=\frac{dL}{dz}\frac{dz}{db}=a-y\\
\\
成本函数对参数w的导:\;\frac{dJ}{dw_n}=\frac{1}{m}\sum_{i=1}^{m}\frac{d}{dw_n}L(a^{(i)},y^{(i)})=\frac{1}{m}\sum_{i=1}^{m}(a^{(i)}-y^{(i)})x_n^{(i)}\\
成本函数对参数b的导:\;\frac{dJ}{db}=\frac{1}{m}\sum_{i=1}^{m}\frac{d}{db}L(a^{(i)},y^{(i)})=\frac{1}{m}\sum_{i=1}^{m}(a^{(i)}-y^{(i)})
\\
参数更新\\
w_n:=w_n-\alpha dw_n\\
b:=b-\alpha db
$$

### 1.2 向量化
<p>&nbsp;&nbsp; 通过上面的推导，我们可以开始设计代码了。因为涉及到许多循环，例如计算成本函数，需要从第1个样本加到第n个样本，我们免不了想到for循环。但是在python中，for循环的代价是比较大的，对于线性回归还好，要是涉及到更深层次的神经网络，或者是想要更多的学习次数，那么for循环带来的延迟是毁灭性的。所以我们要另辟蹊径，不使用显式的for循环，转而使用向量化方法。下面是一个例子。 </p><br/>

```python
#encoding=utf-8
import time
import numpy as np
import math

a = np.random.rand(10000000)
tic = time.time()
map(lambda x: math.exp(x), a)
toc = time.time()
print "map version: " + str(1000*(toc-tic)) + "ms"
# map version: 1753.00002098ms
tic = time.time()
np.exp(a)
toc = time.time()
print "vectorization version: " + str(1000*(toc-tic)) + "ms"
# vectorization version: 85.000038147ms

```
<p>&nbsp;&nbsp; 可见同样的计算量，使用向量化方法比for循环速度相差了20倍左右。这20倍可能就是1小时与1天的差别。所以我们在机器学习中尽量要使用向量化方法。对于线性回归，我们可以这样向量化。 </p><br/>

$$
正向传播
\\
X=
\left[
 \begin{matrix}
   x_1^{(1)} & x_1^{(2)} & x_1^{(3)} & x_1^{(4)} & ...\\
   x_2^{(1)} & x_2^{(2)} & x_2^{(3)} & x_2^{(4)} & ... \\
   x_3^{(1)} & x_3^{(2)} & x_3^{(3)} & x_3^{(4)} & ... \\
   ... & ... & ... & ... & ...
  \end{matrix}
\right]
\;\;
W=
\left[
 \begin{matrix}
   w_1 \\
   w_2 \\
   w_3 \\
   ...
  \end{matrix}
\right]
\\
Z=W^TX+b=
\left[
 \begin{matrix}
   z^{(1)} & z^{(2)} & z^{(3)} & z^{(4)} & ...
  \end{matrix}
\right]
\;\;
A=sigmoid(z)=
\left[
 \begin{matrix}
   a^{(1)} & a^{(2)} & a^{(3)} & a^{(4)} & ...
  \end{matrix}
\right]
$$

<br/>

$$
反向传播
\\
dZ=\frac{dL}{dz}=
\left[
 \begin{matrix}
   a^{(1)}-y^{(1)} & a^{(2)}-y^{(2)} & a^{(3)}-y^{(3)} & a^{(4)}-y^{(4)} & ...
  \end{matrix}
\right]
=A-Y
\\
dW=\frac{dJ}{dw}=\frac{1}{m}X(dZ)^T\;\;
dB=\frac{dJ}{db}=np.sum(dZ, axis=0)
$$

<br/>
<p>&nbsp;&nbsp;这样我们就完成了一个简单的线性回归。</p><br/>

```python
#encoding=utf-8
import numpy as np

def sigmoid(z):
    return 1.0 / (1+np.exp(-z))

def initialize(shape):
    W = np.zeros((shape, 1))
    b = 0
    return W, b

def propogate(X, W, b, Y, alpha, times):
    '''
    参数：X是所有样本，W是参数，b是参数，Y是样本结果
    alpha是学习率，times是学习次数
    '''
    m = X.shape[1]
    J = []
    for i in range(times):
        Z = np.dot(W.T, X) + b      # (1, m)
        A = sigmoid(Z)              # (1, m)
        L = -(Y*np.log(A)+(1-Y)*np.log(1-A))
        J.append((1.0/m)*np.sum(L, axis=1))
        dZ = A - Y                  # (1, m)
        dW = (1.0/m)*np.dot(X, dZ.T)    # (n_X, 1)
        db = (1.0/m)*np.sum(dZ, axis=0)

        W = W - alpha*dW
        b = b - alpha*db
    return J, W, b[0]           # J用来画图，W，b是训练好的参数

def test(test_x, test_y, W, b):
    Z = np.dot(W.T, test_x) + b
    A = sigmoid(Z)
    for i in range(A.shape[1]):
        A[0, i] = 1 if A[0, i] >= 0.5 else 0
    print "测试集准确性为: {}%".format(100-np.mean(np.abs(A-test_y))*100)

```

最终结果，测试集的准确性为68%。成本函数图像如下所示：
<div align="center">
<img src="{{site.url}}{{site.baseurl}}{{site.assets_path}}/img/deeplearning/Figure_1.png" width="50%" height="50%"/>
</div>

<br/>

## 2. Shallow Neural Network
<p>&nbsp;&nbsp; 下面来看一下浅层神经网络。其实对于浅层网络与深层网络划分的界限并不明显，有说法是3层以上的网络就算是深层网络了。而前面提到的线性回归可以看作是只有一个输出层的浅层网络。对于神经网络层数的规定是这样的：数据的输入部分是输入层，输出的部分是输出层，中间的部分是隐藏层。一个网络的层数由隐藏层与输出层决定，输入层不算在层数之中。这个也比较好理解，毕竟输入层只是输入数据，并没有参与运算。对于层数的标注我们引出以下的规定： </p><br/>

$$
a^{[0](1)}: 第一个样本第零层神经网络的输出(输入层)\\
z^{[1](1)}: 第一个样本第一层神经网络的线性组合\\
z_1^{[1](1)}: 第一个样本第一层神经网络的第一个神经元的线性组合\\
$$

<br/>

### 2.1 正向传播
<p>&nbsp;&nbsp; 以两层神经网络为例；隐藏层有四个神经元，输出层有一个神经元。 </p><br/>

$$
正向传播\\
z_1^{[1]}=w_1^{[1]T}X+b_1^{[1]}\;\;\;\;a_1^{[1]}=g^{[1]}(z_1^{[1]})\\
z_2^{[1]}=w_2^{[1]T}X+b_2^{[1]}\;\;\;\;a_2^{[1]}=g^{[1]}(z_2^{[1]})\\
z_3^{[1]}=w_3^{[1]T}X+b_3^{[1]}\;\;\;\;a_3^{[1]}=g^{[1]}(z_3^{[1]})\\
z_4^{[1]}=w_4^{[1]T}X+b_4^{[1]}\;\;\;\;a_4^{[1]}=g^{[1]}(z_4^{[1]})\\
\\
z^{[2]}=w^{[2]T}A^{[1]}+b^{[2]}\;\;\;\;a^{[2]}=g^{[2]}(z^{[2]})\\
$$

<br/>

我们对它进行向量化

$$
计算图\\
X(n,m),W^{[1]}(4,n),b^{[1]}(4,1)\rightarrow Z^{[1]}=W^{[1]}X+b^{[1]}\rightarrow A^{[1]}=g^{[1]}(Z^{[1]})\\
A^{[1]}(4,m),W^{[2]}(1,4),b^{[2]}(1,1)\rightarrow Z^{[2]}=W^{[2]}A^{[1]}+b^{[2]}\rightarrow A^{[2]}=g^{[2]}(Z^{[2]})\rightarrow L(A^{[2]}, Y)
\\
X=
\left[
 \begin{matrix}
   x_1^{(1)} & x_1^{(2)} & x_1^{(3)} & x_1^{(4)} & ...\\
   x_2^{(1)} & x_2^{(2)} & x_2^{(3)} & x_2^{(4)} & ... \\
   x_3^{(1)} & x_3^{(2)} & x_3^{(3)} & x_3^{(4)} & ...
  \end{matrix}
\right]
\;\;
W^{[1]}=
\left[
 \begin{matrix}
   w_11^{[1]} & w_12^{[1]} & w_13^{[1]}\\
   w_21^{[1]} & w_22^{[1]} & w_23^{[1]}\\
   w_31^{[1]} & w_32^{[1]} & w_33^{[1]}\\
   w_41^{[1]} & w_42^{[1]} & w_43^{[1]}
  \end{matrix}
\right]
\;\;
b^{[1]}=
\left[
 \begin{matrix}
   b_1^{[1]}\\
   b_2^{[1]}\\
   b_3^{[1]}\\
   b_4^{[1]}
  \end{matrix}
\right]
\\
W^{[2]}=
\left[
 \begin{matrix}
   w_1^{[2]} & w_2^{[2]} & w_3^{[2]} & w_4^{[2]}
  \end{matrix}
\right]
\;\;
b^{[2]}=b\\
Z^{[1]}=
\left[
 \begin{matrix}
   z_1^{[1](1)} & z_1^{[1](2)} & z_1^{[1](3)} & ...\\
   z_2^{[1](1)} & z_2^{[1](2)} & z_2^{[1](3)} & ...\\
   z_3^{[1](1)} & z_3^{[1](2)} & z_3^{[1](3)} & ...\\
   z_4^{[1](1)} & z_4^{[1](2)} & z_4^{[1](3)} & ...
  \end{matrix}
\right]
\;\;
A^{[1]}=g^{[1]}(Z^{[1]})\\
Z^{[2]}=
\left[
 \begin{matrix}
   z^{[2](1)} & z^{[2](2)} & z^{[2](3)} & z^{[2](4)} & ...
  \end{matrix}
\right]
\;\;
A^{[2]}=g^{[2]}(Z^{[2]})\\
$$

<br/>

### 2.2 反向传播
<p>&nbsp;&nbsp; 反向传播还是要用到链式求导。注意选择合适的损失函数与成本函数，并且求得的每一个参数的导数矩阵的型一定与参数矩阵的型一致。 </p><br/>

$$
设损失函数仍是
L=-(ylog(a)+(1-y)log(1-a))\\
dA^{[2]}=\frac{dL}{dA^{[2]}}=\frac{1-Y}{1-A^{[2]}}-\frac{Y}{A^{[2]}}\\
dZ^{[2]}=\frac{dL}{dZ^{[2]}}=\frac{dL}{dA^{[2]}}\frac{dA^{[2]}}{dZ^{[2]}}=dA^{[2]}*g^{[2]'}(Z^{[2]})\\
\frac{dL}{dW^{[2]}}=dZ^{[2]}A^{[1]T}\\
\frac{dL}{db^{[2]}}=dZ^{[2]}\\
dA^{[1]}=\frac{dL}{dA^{[1]}}=W^{[2]T}dZ^{[2]}\\
dZ^{[1]}=dA^{[1]}*g^{[1]'}(Z^{[1]})\\
\frac{dL}{dW^{[1]}}=dZ^{[1]}X^{T}\\
\frac{dL}{db^{[1]}}=dZ^{[1]}
\\
dW^{[2]}=\frac{1}{m}dZ^{[2]}A^{[1]T}\;\;
dW^{[1]}=\frac{1}{m}dZ^{[1]}X^{T}\\
db^{[2]}=\frac{1}{m}np.sum(dZ^{[2]},axis=1,keepdims=True)\;\;
db^{[1]}=\frac{1}{m}np.sum(dZ^{[1]},axis=1,keepdims=True)
$$

<br/>

### 2.3 激活函数
<p>&nbsp;&nbsp; 首先为什么要使用激活函数呢？如果不使用激活函数的话，那么算来算去最终的结果都是线性组合。既然如此，多层网络的意义又在哪里呢？所以我们要使用激活函数来为网络添加一点非线性的元素进去。这样网络才能计算出更加有意思的函数来。 </p><br/>

| 激活函数 | 表达式 | 备注 |
|---|---|---|
| sigmoid | $$a=\frac{1}{1+e^{-z}}$$ | 可以将输出限制到0~1，但是在值很大或很小的情况下，斜率趋向于0，学习速度缓慢。 |
|---|---|---|
| tanh | $$a=\frac{e^z-e^{-z}}{e^z+e^{-z}}$$ | 输出限制在-1~1，效果优于sigmoid，尽量替代sigmoid(概率问题除外)，缺点与sigmoid相同 |
|---|---|---|
| relu | $$a=max(0,z)$$ | 不会出现梯度消失的问题，在神经网络中十分常用 |
|---|---|---|
| leaky relu | $$a=max(0.01z,z)$$ | relu的改良版 |

<br/>

### 2.4 初始化问题
<p>&nbsp;&nbsp; 在线性回归中，可以将W与b初始化为全0，但是在神经网络中不能这么做。这样会导致所有神经元一直计算相同的函数，就失去了多个神经元的意义。所以一般将W与b初始化为随机数。如果使用sigmoid，tanh激活函数，还要初始化为很小的随机数，因为在0附近才能获得更大的斜率，更好的学习效率。 </p><br/>

### 2.5 浅层网络代码

```python
def tanh(Z):
    a = (np.exp(Z)-np.exp(-Z))/(np.exp(Z)+np.exp(-Z))
    return a

def layer_size(X, Y):
    '''
    给出合适的layer size
    '''
    n_x = X.shape[0]
    n_y = Y.shape[0]
    return n_x, n_y

def initialize(n_x, n_h, n_y):
    W1 = np.random.randn(n_h, n_x)*0.01     # 因为使用tanh激活函数
    b1 = np.zeros((n_h, 1))
    W2 = np.random.randn(n_y, n_h)*0.01     # 因为使用sigmoid激活函数
    b2 = np.zeros((n_y, 1))
    para = {"W1": W1, "W2": W2, "b1": b1, "b2": b2}
    return para

def forward_prop(X, para):
    m = X.shape[1]
    Z1 = np.dot(para["W1"], X)+para["b1"]
    A1 = tanh(Z1)
    Z2 = np.dot(para["W2"], A1)+para["b2"]
    A2 = sigmoid(Z2)
    cache = {"A2": A2, "A1": A1, "Z2": Z2, "Z1": Z1, "m": m}
    return cache

def costs(cache, J):
    L = -(Y * np.log(cache["A2"]) + (1 - Y) * np.log(1 - cache["A2"]))
    J.append((1.0 / cache["m"]) * np.sum(L, axis=1))

def back_prop(cache, X, Y, para, alpha):
    dZ2 = cache["A2"] - Y
    dW2 = (1.0/cache["m"])*np.dot(dZ2, cache["A1"].T)
    db2 = (1.0/cache["m"])*np.sum(dZ2, axis=1, keepdims=True)
    dZ1 = np.dot(para["W2"].T, dZ2)*(1 - np.power(cache["A1"], 2))
    dW1 = (1.0/cache["m"]) * np.dot(dZ1, X.T)
    db1 = (1.0/cache["m"]) * np.sum(dZ1, axis=1, keepdims=True)

    para["W1"] -= alpha*dW1
    para["W2"] -= alpha*dW2
    para["b1"] -= alpha*db1
    para["b2"] -= alpha*db2

def test(X, Y, para):
    cache = forward_prop(X, para)
    predict = cache["A2"]
    for i in range(predict.shape[1]):
        predict[0, i] = 1 if predict[0, i] >= 0.5 else 0
    print "测试集准确性为: {}%".format(100 - np.mean(np.abs(predict - Y)) * 100)

def nn_model(X, Y, n_h, num_iteration, alpha):
    n_x, n_y = layer_size(X, Y)
    para = initialize(n_x, n_h, n_y)
    J = []
    for i in range(num_iteration):
        cache = forward_prop(X, para)
        costs(cache, J)
        back_prop(cache, X, Y, para, alpha)
    plt.plot(J)
    pylab.show()
    return para

```
<br/>

最终结果，测试集的准确性为90%。成本函数图像如下所示：

<div align="center">
<img src="{{site.url}}{{site.baseurl}}{{site.assets_path}}/img/deeplearning/Figure_2.png" width="50%" height="50%"/>
</div>

## 3. Deep Neural Network
<p>&nbsp;&nbsp;上面看到了浅层神经网络的表现，是非常不错的。不但能学习线性回归学不了的函数，精度还有了进一步的提升。那么下面就探究深层神经网络的表现。</p><br/>

### 3.1 为什么要使用深层网络
<p>&nbsp;&nbsp; 从线性回归到浅层神经网络的例子我们已经看出，多了一层网络就能计算更加复杂的函数了。深层网络也是如此。在浅层的部分会计算一些简单的函数，而到了深层部分，函数会越来越复杂。就像阿柴的分类。可能浅层网络会提取一些特征，例如直线，曲线等，而到了深层网络则是这些特征的组合，例如鼻子，眼睛等。所以深层网络具有巨大的研究价值。 </p><br/>

### 3.2 正向传播
<p>&nbsp;&nbsp;以四层的网络为例，三层隐藏层，一层输出层。其中前两层隐藏层均为5个节点，后一层隐藏层3个节点，输出层一个节点。</p><br/>

$$
Z^{[1]}=W^{[1]}X+b{[1]}\;\;A^{[1]}=g^{[1]}(Z^{[1]})\\
Z^{[2]}=W^{[2]}A^{[1]}+b{[2]}\;\;A^{[2]}=g^{[2]}(Z^{[2]})\\
Z^{[3]}=W^{[3]}A^{[2]}+b{[3]}\;\;A^{[3]}=g^{[3]}(Z^{[3]})\\
Z^{[4]}=W^{[4]}A^{[3]}+b{[4]}\;\;A^{[4]}=g^{[4]}(Z^{[4]})
$$

<br/>

我们对它进行向量化

$$
计算图\\
X(n,m),W^{[1]}(5,n),b^{[1]}(5,1)\rightarrow Z^{[1]}=W^{[1]}X+b^{[1]}\rightarrow A^{[1]}=g^{[1]}(Z^{[1]})\\
A^{[1]}(5,m),W^{[2]}(5,5),b^{[2]}(5,1)\rightarrow Z^{[2]}=W^{[2]}A^{[1]}+b^{[2]}\rightarrow A^{[2]}=g^{[2]}(Z^{[2]})\\
A^{[2]}(5,m),W^{[3]}(3,5),b^{[3]}(3,1)\rightarrow Z^{[3]}=W^{[3]}A^{[2]}+b^{[3]}\rightarrow A^{[3]}=g^{[3]}(Z^{[3]})\\
A^{[3]}(3,m),W^{[4]}(1,3),b^{[4]}(1,1)\rightarrow Z^{[4]}=W^{[4]}A^{[3]}+b^{[4]}\rightarrow A^{[4]}=g^{[4]}(Z^{[4]})\\
A^{[4]}(1,m)\rightarrow L(A^{[4]}, Y)\\
写成一般形式(令X=A^{[0]})\;i=[0,3]\\
A^{[i]},W^{[i+1]},b^{[i+1]}\rightarrow Z^{[i+1]}=W^{[i+1]}A^{[i]}+b^{[i+1]}\rightarrow A^{[i+1]}=g^{[i+1]}(Z^{[i+1]})
$$

<br/>

### 3.3 反向传播
<p>&nbsp;&nbsp; 同样需要链式求导，规律可以从浅层网络中寻得。 </p><br/>

$$
dA^{[4]}=\frac{dL}{dA^{[4]}}\;\;\;\;dZ^{[4]}=dA^{[4]}\frac{dA^{[4]}}{dZ^{[4]}}=dA^{[4]}*g^{[4]'}(Z^{[4]})\\
\frac{dL}{dW^{[4]}}=dZ^{[4]}\frac{dZ^{[4]}}{dW^{[4]}}=dZ^{[4]}A^{[3]T}\;\;\;\;dW^{[4]}=\frac{1}{m}dZ^{[4]}A^{[3]T}\\
\frac{dL}{db^{[4]}}=dZ^{[4]}\;\;\;\;db^{[4]}=\frac{1}{m}np.sum(dZ^{[4]}, axis=1, keepdims=True)\\
dA^{[3]}=W^{[4]T}dZ^{[4]}\\
...\\
得出规律\\
dW^{[l]}=\frac{1}{m}dZ^{[l]}A^{[l-1]T}\\
db^{[l]}=\frac{1}{m}np.sum(dZ^{[l]}, axis=1, keepdims=True)\\
dA^{[l-1]}=W^{[l]T}dZ^{[l]}\\
$$

<br/>

### 3.4 深层网络代码

```python
def initialize(layers_dims):
    '''
    layers_dims包含了每层网络的神经元个数(从输入层开始)
    '''
    L = len(layers_dims)
    para = {}
    for i in range(1, L):
        para["W"+str(i)] = np.random.randn(layers_dims[i], layers_dims[i-1]) / np.sqrt(layers_dims[i-1])
        para["b"+str(i)] = np.zeros((layers_dims[i], 1))

    return para

def forward_prop(A_pre, W, b, activation):
    '''
    正向传播
    输入上一层的A，本层的W，b
    输出本层的A，Z(为了反向传播)
    '''
    Z = np.dot(W, A_pre) + b
    if activation == "sigmoid":
        A = sigmoid(Z)
    elif activation == "relu":
        A = relu(Z)
    return A, Z

def cost_funtion(A, Y, J):
    L = -(Y*np.log(A)+(1-Y)*np.log(1-A))
    J.append((1.0/A.shape[1])*np.sum(L, axis=1))

def back_prop(dA, activation, Z, A_pre, para, alpha, i):
    if activation == "sigmoid":
        dZ = sigmoid_backward(dA, Z)
    elif activation == "relu":
        dZ = relu_backward(dA, Z)
    dW = (1.0/dA.shape[1]) * np.dot(dZ, A_pre.T)
    db = (1.0/dA.shape[1]) * np.sum(dZ, axis=1, keepdims=True)
    dA_pre = np.dot(para["W"+str(i)].T, dZ)

    para["W"+str(i)] -= alpha*dW
    para["b"+str(i)] -= alpha*db

    return dA_pre

def test(X, Y, para, layers_dims, activations):
    Zs = {}
    As = {"A0": X}
    for j in range(1, len(layers_dims)):
        As["A" + str(j)], Zs["Z" + str(j)] = forward_prop(As["A" + str(j - 1)], para["W" + str(j)], para["b" + str(j)],
                                                          activations[j - 1])
    predict = As["A" + str(len(layers_dims) - 1)]
    for i in range(predict.shape[1]):
        predict[0, i] = 1 if predict[0, i] >= 0.5 else 0
    print ("测试集准确性为: {}%".format(100 - np.mean(np.abs(predict - Y)) * 100))


def nn_model(X, Y, layers_dims, activations, alpha, num_iterations):
    '''
    X为输入层，Y为输出层，layers_dims为每一层的神经元个数，activations为每一层的激活函数
    alpha为学习率，num_iterations为迭代次数
    '''
    layers_dims.insert(0, X.shape[0])
    para = initialize(layers_dims)
    J = []
    Zs = {}
    As = {"A0": X}
    for i in range(num_iterations):
        for j in range(1, len(layers_dims)):
            As["A"+str(j)], Zs["Z"+str(j)] = forward_prop(As["A"+str(j-1)], para["W"+str(j)], para["b"+str(j)], activations[j-1])
        # As为本轮正向传播A的集合，Zs为本轮正向传播Z的集合
        A = As["A"+str(len(layers_dims)-1)]
        cost_funtion(A, Y, J)
        dA = np.divide(1-Y, 1-A)-np.divide(Y, A)
        for j in range(len(layers_dims)-1, 0, -1):
            dA = back_prop(dA, activations[j-1], Zs["Z"+str(j)], As["A"+str(j-1)], para, alpha, j)

    plt.plot(J)
    pylab.show()
    return para

```
<br/>

### 3.5 再谈初始化
<p>&nbsp;&nbsp; 前面提到了线性回归的初始化问题，以及浅层神经网络的初始化问题。其实对于不同的激活函数，参数矩阵的初始化也是不一样的。我们需要保证随着网络层数的增加，后边网络得到的输入也是可以服从标准高斯分布的(为了防止梯度消失)。所以对于不同的激活函数，我们要采取针对性的初始化方式， </p><br/>

| 激活函数 | 初始化方式 |
|---|---|
| sigmoid() | np.random.randn(x, y) * 0.01 |
|---|---|
| tanh() | np.random.randn(x, y) * np.sqrt(1/y) |
|---|---|
| relu() | np.random.randn(x, y) * np.sqrt(2/y) |

<br/><br/>