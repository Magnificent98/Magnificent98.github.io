---
layout: post
title: Deep Learning(III)
category: Deep Learning
date: 2020-6-8
---

# 这章针对卷积网络

## 1. 卷积神经网络
<p>&nbsp;&nbsp; 卷积神经网络在图片分类，物体探测，风格转换方面有重要作用。 </p><br/>

### 1.1 卷积运算与互相关运算
<p>&nbsp;&nbsp; 二维卷积运算相当于把卷积核先上下颠倒，再左右颠倒，然后对应位置相乘得到一个数值。但是在卷积神经网络中，一般不这么干，因为颠倒矩阵需要额外的开销。在卷积神经网络中是使用互相关运算代替卷积的，即对应位置直接相乘然后相加。这样对结果是没有影响的。但是两种运算要区分开来。 </p><br/>

### 1.2 卷积核
<p>&nbsp;&nbsp; 卷积核是卷积网络的核心。它是一个NxN的二维矩阵，与上一层的输出进行互相关运算。卷积核被设计的初衷是提取图像中的一些特征，比如边缘，棱角等。例如下面这个卷积核可以提取图像中的竖直边缘。 </p><br/>

$$

\left[
 \begin{matrix}
   1 & 0 & -1\\
   1 & 0 & -1\\
   1 & 0 & -1
  \end{matrix}
\right]

$$

<br/>

<p>&nbsp;&nbsp; 理解起来也很简单，如果图像靠左边一点比靠右边一点数值大，那么卷积运算之后，数值也大，即记录下了竖直的边缘。在神经网络中，卷积核是交给网络通过反向传播自己更新的，不用自己设计。 </p><br/>

### 1.3 卷积神经网络层的参数
<p>&nbsp;&nbsp; 一般一层卷积神经网络层得到x*y*c的输入会给出a*b*c的输出。这三个参数可能统统不一样，是由卷积核大小(kernal_size)，步长(stride)，卷积核个数，填充(padding)决定的。 </p><br/>
<p>&nbsp;&nbsp; 填充是指将待卷的特征图边缘加上一些数据，可以是0也可以是其他。步长是卷积核移动的步子大小，可以一次移动一格，或者多格。它们与卷积核大小共同决定了输出特征图的尺寸。而卷积核的个数决定了输出特征图的通道数。 </p><br/>

$$
设输入特征图大小为NxN，填充大小为P，步长为S，卷积核大小为f，个数为m\\
输出特征图的尺寸为\\
math.floor(\frac{N+2P-f}{S}+1)，通道数为m
$$

<br/>

### 1.4 单层卷积网络
<p>&nbsp;&nbsp; 下面来看看一层卷积网络是怎么工作的。 </p><br/>

| 输入 | 卷积核 | 步长 | 填充 | 卷积核个数 | 输出 |
|---|---|---|---|---|---|
| 6x6x3 | 3x3 | 1 | 0(valid) | 2 | 4x4x2 |
|---|---|---|---|---|---|
| 6x6x3 | 3x3 | 1 | 1(same) | 2 | 6x6x2 |

<br/>

<p>&nbsp;&nbsp; 因为输入是3通道的，所以卷积核复制成3通道，然后对每个通道进行卷积，得到3个值，再将3个值相加，得到对应中心位置的卷积值。之后滑动卷积核得到4x4的输出，加上一个bias，过一下激活函数就完成了这一个卷积核的卷积工作。第二个卷积核照搬操作，然后将两个卷积后的输出在通道的维度上连接，就得到了最终的输出。 </p><br/>

### 1.5 多层卷积网络结构
<p>&nbsp;&nbsp; 将多个单层卷积网络拼在一起就能得到多层卷积网络。而根据最终的用途，又可以连接上全连接层，或干脆是全卷积网络。下面给出一种可能的卷积网络结构。 </p><br/>

```python
input = 39*39*3
conv2d(in_features=3, out_features=10, kernal_size=3, stride=1, padding=0)
out1 = 37*37*10
conv2d(in_features=10, out_features=20, kernal_size=5, stride=2, padding=0)
out2 = 17*17*20
conv2d(in_features=20, out_features=40, kernal_size=5, stride=2, padding=0)
out3 = 7*7*40
flatten out3(1960)
linear(in_features=1960, out_features=1)
# 可见特征图尺寸一直在下降，通道数一直在上升

```
<br/>

### 1.6 池化层
<p>&nbsp;&nbsp; 池化层名字起的好怪，不如叫压缩层。它的作用是，用一个“卷积核”去扫荡特征图，然后将核里的最大值或者是均值返回出来。即用一个点来代替一片区域。最终的结果就是图像尺寸被压缩了。 </p><br/>

| 输入尺寸 | “池化核” | 步长 | 输出 |
|---|---|---|---|
| 5x5x2 | 3x3 | 1 | 3x3x2 |
|---|---|---|---|---|---|
| 28x28x6 | 2x2 | 2 | 14x14x6 |

<br/>

### 1.7 为什么使用卷积网络
<p>&nbsp;&nbsp; 首先在计算方式上，卷积网络也是遵循先线性，后激活的套路。那么为什么不直接使用普通的全连接神经网络呢？ </p><br/>

<p>(1).参数共享：已知卷积核可以提取特征，那么同一个卷积核就应该可以重复使用。这样重复使用的特性大大减少了网络训练的参数。</p><br/>
<p>(2).稀疏连接：每一部分的输出只与一小部分输入有关，这样做相比于全连接网络相当于简化了网络结构，有助于防止过拟合。</p><br/>

## 2. 残差网络
<p>&nbsp;&nbsp; 网络的发展趋势是越来越深。深有深的好处的，比如对于卷积神经网络，浅层可能在计算一些线条特征，而到了深层可能在计算一些组合特征(眼睛，鼻子)，这样整个网络的抽象能力就更好一点。但是网络深了也会有问题。因为权值都是比较小的，深层网络的梯度可能传播不到浅层，也有极少数的情况下，权值很大，浅层网络的梯度直接爆炸。这样随着网络层数的加深，误差可能不降反增。残差网络就是为了解决这个问题的。 </p><br/>

<p>&nbsp;&nbsp; 残差网络的特色是增加了短路连接。即当前层的输出可以直接给到后面几层。这样的效果就是，即便整体网络加深，因为有了短路层的存在，浅层还是能获得梯度的更新。并且使用残差网络是保证性能不会更差的。因为被短路的几层网络可以看作是bonus，即便它们没有学到任何东西，参数全部为0，那么前面的数据还是可以在网络中传播下去；而如果它们学到了任何东西，那么对于网络来说肯定是有益的。 </p><br/>

<p>&nbsp;&nbsp; 残差网络允许我们使用更深层的神经网络。具体的实现在上一章。 </p><br/>

## 3. Inception Network
<p>&nbsp;&nbsp; 小朋友，你是否有很多问号，为什么我们要自己选卷积核的大小？让网络自己决定不是更好吗？于是出现了inception network。 </p><br/>

| 输入 | 卷积核 | 输出 |
|---|---|---|
| 28x28x192 | 1x1 64个<br/>3x3 same padding 128个<br/>5x5 same padding 32个<br/>maxpooling same padding 32个 | 28x28x256 |

<br/>

<p>&nbsp;&nbsp; 这个样子，我们一层网络经过了4中卷积核的洗礼。哪种效果更好，网络自然就会偏向那种啦。但是也会带来个问题，那就是计算量爆炸。以5x5的卷积核为例。 </p><br/>


$$
对于5x5的卷积核，一共产生的计算量为：\\
(28*28*32)*(5*5*192)=120M
$$

<br/>

<p>&nbsp;&nbsp; 这么大的计算量是比较恐怖的。仔细观察，是通道数出了问题，输入有192个通道，导致了计算量的爆炸。这里我们可以压缩一下通道数解决问题。 </p><br/>

| 输入 | 卷积核1 | 中间输出 | 卷积核2 | 输出 |
|---|---|---|---|---|
| 28x28x192 | 1x1 16个 | 28x28x16 | 5x5 samepad 32个 | 28x28x32 |

<br/>

<p>&nbsp;&nbsp; 使用1x1的卷积核可以压缩输入的通道数。这样可以使计算量大大减少。 </p><br/>

$$
对于5x5的卷积核，一共产生的计算量为：\\
(28*28*32)*(5*5*16)=12.4M
$$

<br/>

<p>&nbsp;&nbsp; 所以，Inception network都是带着这样的中间层(bottleneck layer)的，先把通道数压缩下来，再进行计算。 </p><br/>

## 4. 迁移学习
<p>&nbsp;&nbsp; 俗话说，站在巨人的肩膀上，可以看的更远。自己的运算资源是有限的，很多时候我们需要借助别人的成果，在别人的成果上发展壮大。这就引出了迁移学习的概念。在开发一个新的机器学习项目时，我们可以查查前人的成果，拿前人训练好的参数作为自己网络的一部分，可以大大减少时间，加快拟合的速度。 </p><br/>


## 5. 目标检测
<p>&nbsp;&nbsp; 目标检测是卷积神经网络的一个重要应用。这里介绍的YOLO算法。 </p><br/>

### 5.1 YOLO基本概念
<p>&nbsp;&nbsp; YOLO的基本想法是将整张图片分割成n*n的小块，对每个小块进行预测。这样整张图片只要扫描一遍，就能给出完成的预测结果。所以叫YOLO(You Only Look Once). </p><br/>
<p>&nbsp;&nbsp; 模型的细节如下：若要在一张图片中寻找80种分类的物品，可以将图片分为19*19的小块，对于每一个小块，给出一个85维的向量进行预测。这85维分别是[P, x, y, h, w, c1, c2, c3, ... ,c80]. P代表小块内有带探测物品的概率，x, y, b, w代表该物品的位置以及大小信息。剩下的80维代表80种物品各自的概率，它们的和为1。所以每个物品的概率计算如下，我们要做的是选出最大概率的那个，作为这个小格子的预测结果。 </p><br/>

<div align="center">
<img src="{{site.url}}{{site.baseurl}}{{site.assets_path}}/img/deeplearning/III_1.png" width="50%" height="50%"/>
</div>

<br/>

<p>&nbsp;&nbsp; 但是这样做也会产生一个问题。难道一个格子内只能有一个物品吗？如果一个格子内有两个怎么办？我们只会选择最大的那个。这样会造成信息的损失。所以引入了锚框(anchor box)的概念。我们可以预先设置几个锚框，如果一个格子内有一种以上的物品，我们就用不同的锚框标记他们。比如一个格子里既探测到了车，又探测到了人，可以用1号锚框标记车，2号锚框标记人。这里我们采用的是5个锚框的设计。如果你问一个格子里超过了5个物体怎么办，那就是格子太大了，把它缩小一点。现在整个模型的结构变成了这样。 </p><br/>

<div align="center">
<img src="{{site.url}}{{site.baseurl}}{{site.assets_path}}/img/deeplearning/III_2.png" width="50%" height="50%"/>
</div>

<br/>

### 5.2 分类阈值过滤
<p>&nbsp;&nbsp; 完成了上面的步骤，可能每个格子都会输出5个锚框。因为我们选择每个锚框概率最大的类别，这个最大值是一定会出现的。但是如果最大值只有0.1，0.2，那么我们可以认为这个锚框的输出是可以忽略的。这时就要过第一道关，分类阈值过滤。比如我们把所有概率小于0.4的过滤掉，那么每个小格子可能就只会输出1个或者2个锚框。 </p><br/>

### 5.3 非最大值抑制
<p>&nbsp;&nbsp; 还有一种情况，两个格子都预测到了一辆车的存在，它们一起给出了车的位置以及尺寸信息，如果两个都采用的话，整个系统就会显得比较杂乱。我们必须抛弃一个。对于两个预测，我们肯定希望保留概率大的。但是删掉那个概率较小的需要的条件是什么呢？万一另一个预测的是其它的车(距离较近)呢？这时引入交并比的概念。 </p><br/>

<div align="center">
<img src="{{site.url}}{{site.baseurl}}{{site.assets_path}}/img/deeplearning/III_3.png" width="50%" height="50%"/>
</div>

<br/>
<p>&nbsp;&nbsp; 如上所示，当交并比超过了一定阈值的时候，我们就可以断定，这两个框框预测的同一辆车，就可以把概率小的那个直接扔掉了。 </p><br/>

### 5.4 整体流程
<p>&nbsp;&nbsp; 对于输入的图像，我们卷积生成19*19*5*85的输出。先经过分类阈值过滤，排除掉每个格子中概率较小的锚框。之后，取出概率最大的框框，删掉与它交并比大于一定阈值的所有框框，画出这个框框。然后再取当前概率最大的框框，反复进行下去。这样就给出了完美的预测结果。 </p><br/>

<div align="center">
<img src="{{site.url}}{{site.baseurl}}{{site.assets_path}}/img/deeplearning/III_4.png" width="50%" height="50%"/>
</div>

<br/>
